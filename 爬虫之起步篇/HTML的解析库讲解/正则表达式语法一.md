# 如何创建一个最简单的爬虫

首先先导入一些库

```python
import re
import requests
```

然后创建一个请求头，请求头的含义是为了伪装成一个浏览器，不然容易被系统检测出来是一个爬虫，然后就很容易被封IP，如何去寻找这些请求头，先打开浏览器，然后鼠标右键点击检查，然后在选项卡选择network，先刷新页面，然后选择第一个文件，在里面寻找你需要用到的信息，不过大多数信息没什么用，一般都是用到user-agent和cookie比较多。

```python
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.62 Safari/537.36",
    "Cookie": "Cookie: ABTEST=0|1589381138|v17; SUID=11129B242513910A000000005EBC0812; SUV=000DEA07656B54065EBC08135838A078; IPLOC=CN4401; browerV=3; osV=1; sst0=866; usid=5860406E9F13A00A000000005ED6495A; SESSION_CAPTCHA=57jqkl3g4j4nbiiak7u39qq0m7; seccodeErrorCount=1|Tue, 02 Jun 2020 12:58:02 GMT; SNUID=97AF90BED0CA77C9AB3FCEA9D0183B3E; seccodeRight=success; successCount=1|Tue, 02 Jun 2020 12:58:12 GMT; taspeed=taspeedexist; pgv_pvi=906772480; pgv_si=s2431237120; ld=Gkllllllll2WslkvlllllVEQvEDlllllJJ7v9yllllUllllllklll5@@@@@@@@@@",
}
```

做好这些准备工作后，我们开始写代码

```python
#先写成函数形式，高内聚低耦合，实现一个函数一个功能
def prose_get(url):#赋予一个参数，方便调用
    #先requests.get请求url这个网页
    html = requests.get(url,headers=headers)
    #status_code表示这个网页的状态码的意思，一般200是正常的意思，400是访问失败，500是服务器问题。
    if html.status_code == 200:
        #如果状态码是200，打印这个网页的源代码，用文本的形式
        r=html.text
        #这里采用的是正则表达式，这句话的意思是打印href里面的url
        res_url = re.compile("(?<=href=\").+?(?=\")|(?<=href=\').+?(?=\')",re.I | re.S)
        #把正则表达式匹配到的结果全部打印出来，findall是用于正则表达式，作用是打印全部内容
        result = res_url.findall(r)
        print(result)
        #print(r)
    else:
        print("error")
```

最后再使用main函数调用这个函数

```python
if __name__ == '__main__':
    url = "https://www.sogou.com/sogou?query=python"
    prose_get(url)
```


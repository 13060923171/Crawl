# 一个简单的实战内容

如何爬取实习僧的信息（这里牵涉到一些反爬机制，毕竟既然有爬取肯定就有反爬，这个斗争从计算机出来就一直存在，越是优秀的爬虫工程师也意味着他也是一个优秀的反爬取工程师）

# 还是先导入一些最基本的东西

```python
import requests
from bs4 import BeautifulSoup

headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.62 Safari/537.36"
}
```

我们先写爬取这个网站职位信息的函数

```python
def crawl():
    #先是爬取多少页，根据网页的形式找规律
    for page in range(1,2):
        #然后进行替换，方便爬取多页信息
        html = requests.get("https://www.shixiseng.com/interns?page={}&keyword=python".format(page),headers=headers)
        soup = BeautifulSoup(html.text,"lxml")
        #爬取我们想要的职位的所有个数
        offers = soup.select(".intern-wrap.intern-item")
        #然后用一个循环叠带把这些职位的里面的URL全部给一个个爬取出来
        for offer in offers:
            url = offer.select(".f-l.intern-detail__job a")[0]["href"]
            #再把这些爬取出来的URL全部传入到detial_url这个函数里面
            detial_url(url)
```

然后创建一个函数用来获取页面信息

```python
def detial_url(url):
    #先是解析页面信息
    html = requests.get(url,headers=headers)
    #然后用解析器获取页面的信息后转化为文本形式
    soup = BeautifulSoup(html.text,"lxml")
    #爬取它的标题，用解析器，然后再用文本的形式保存下来
    title = soup.title.text
    #爬取它的公司名字，先用解析器的select方法进行爬取，爬取多个是用select，爬取一个是用select_one，括号里面的是定位语句，这个就涉及到bs4和xpath语法了到时候会有相应一期的笔记，然后用[0]定位爬取的内容
    company_name =soup.select(".com_intro .com-name ")[0].text
    #爬取它的工作，encode("utf-8")是它的编码相对于计算机的ASCII，只是utf-8更全而已，因为实习僧在工作里面加了反爬机制，所以我们得破解它的反爬机制才行，不然会出现乱码
    salary= soup.select(".job_money.cutom_font")[0].text.encode("utf-8")
    #怎么解码呢,参考下面一条语句，把破解之后的东西和最初的文字一一对应就好了
    salary = salary.replace(b"\xef\x92\xa6",b"1")
    salary = salary.replace(b"\xef\x84\xbf",b"5")
    salary = salary.replace(b"\xef\x84\xbf",b"0")
    #有编码自然有解码，因为我们上面这些都是把他们转化为utf-8格式，这不是我们想要的内容，所以我用到了decode用于解码，还原我们想要的内容，最后打印这些内容
    salary = salary.decode()
    print(title,company_name,salary)
```

```python
print("要破解的内容".encode("utf-8"))
```

最后再调用crawl函数开始爬取内容，就这样一个简单的实战就完成了